[{"content":" Answer originally suggested by Jack Penzer, Senior Data Scientist The Problem Sometimes, the plots produced by packages such as SegmentR appear squished with not much white space surrounding the axis text (i.e.¬†the terms for each topic).\nLet‚Äôs use the example ‚ÄòTop Terms‚Äô plot from the SegmentR documentation to see what this means. We start by quickly running through a very simple topic modelling workflow (NB we are not trying different values of min_freq, k_opts etc here because this is just for demonstration- we must try different values for project work)\nsprinklr_export \u0026lt;- SegmentR::sprinklr_export %\u0026gt;% filter(!str_detect(Message, \u0026quot;A word of advice\\\\?\u0026quot;)) dtms \u0026lt;- sprinklr_export %\u0026gt;% SegmentR::make_DTMs(text_var = Message, min_freq = 10, hashtags = FALSE, mentions = FALSE, clean_text = TRUE, remove_stops = TRUE) ldas \u0026lt;- dtms %\u0026gt;% SegmentR::fit_LDAs(k_opts = 3, iter_opts = 1000) explore \u0026lt;- ldas %\u0026gt;% SegmentR::explore_LDAs(top_terms = TRUE, diff_terms = TRUE, bigrams = TRUE, exemplars = TRUE) Now, we can view this plot by pulling the top_terms and plucking ‚Äúall_terms‚Äù (again, this is all explained in the SegmentR documentation)\nexplore %\u0026gt;% filter(freq_cutoff == 10 \u0026amp; k == 3 \u0026amp; iter == 1000) %\u0026gt;% pull(top_terms) %\u0026gt;% pluck(1, \u0026quot;all_terms\u0026quot;) We can see that all of the text on the y axis (the top terms for each topic) is a tad squished and doesn‚Äôt look the best.\nThe Solution There are a few different solutions which can be tried to fix this problem. The first is to save the plot using JPackage::plot_save() with larger dimensions. Please read the Slack Overflow page for this here to see the code and rationale behind this.\nIf this doesn‚Äôt work, or if we want a different solution we can manipulate the plot by either: 1) reducing the number of terms shown, 2) laying out the plots on only one row, 3) reducing the font size\nLet‚Äôs look at these solutions one by one.\nTo reduce the number of terms shown, we need to go back to our SegmentR::explore_LDAs() function. There is an argument here, top_n, which determines how many terms per topic we visualise. For a full list of potential arguments, you can type ?SegmentR::explore_LDAs to access and read the documentation.\nLet‚Äôs change this argument to 12 and see if this helps (note as seen in the documentation, the default value for this is 20):\nexplore \u0026lt;- ldas %\u0026gt;% SegmentR::explore_LDAs(top_terms = TRUE, diff_terms = TRUE, bigrams = TRUE, exemplars = TRUE, top_n = 12) #Set the number of terms to appear as 12 explore %\u0026gt;% filter(freq_cutoff == 10 \u0026amp; k == 3 \u0026amp; iter == 1000) %\u0026gt;% pull(top_terms) %\u0026gt;% pluck(1, \u0026quot;all_terms\u0026quot;) Et voil√†, you can see that now we have reduced the number of terms, there is nicer white space between the bars and the text isn‚Äôt so squished.\nWe can also use the SegmentR::explore_LDAs function to force the plot to appear in one row (rather than faceting the plot into two rows of charts, as it is currently doing). Again, this is controlled by the argument nrow and we can set this to be equal to 1: explore \u0026lt;- ldas %\u0026gt;% SegmentR::explore_LDAs(top_terms = TRUE, diff_terms = TRUE, bigrams = TRUE, exemplars = TRUE, nrow = 1) #Set the number of rows to be 1 explore %\u0026gt;% filter(freq_cutoff == 10 \u0026amp; k == 3 \u0026amp; iter == 1000) %\u0026gt;% pull(top_terms) %\u0026gt;% pluck(1, \u0026quot;all_terms\u0026quot;) As we are not forcing the plots on two rows, the stretch out vertically a bit more now, but certainly do not seem as squashed in this dimension any more.\nFinally, we can edit the size of the text in question specifically. In contrast to the previous two solutions, we do not do this via the SegmentR::explore_LDAs function, but rather explicitly set this when pulling the plot from the explore object. We do this by providing the ggplot2 function theme() to our code. Within theme() we can specify the argument axis.text.y to be equal to element_text(size = 8). Play around with this size (larger numbers = larger text) until you are happy. Note that this can also be used to specify the size of the x axis text with the argument axis.text.y. As always though, if you end up changing the size of the axis text in one plot, be considerate as to whether other plots also need to be changed for consistency. explore \u0026lt;- ldas %\u0026gt;% SegmentR::explore_LDAs(top_terms = TRUE, diff_terms = TRUE, bigrams = TRUE, exemplars = TRUE) explore %\u0026gt;% filter(freq_cutoff == 10 \u0026amp; k == 3 \u0026amp; iter == 1000) %\u0026gt;% pull(top_terms) %\u0026gt;% pluck(1, \u0026quot;all_terms\u0026quot;) + theme(axis.text.y = element_text(size = 8)) #Use the theme function to specify the size of the y axis text And there you go, these are some potential solutions to helping clean up some plots that have been made in SegmentR. Note that these are not mutually exclusive, and we can combine these solutions together (for example displaying 12 terms, on one row, with a smaller font size) if we so wish.\n","permalink":"https://slack-overflow-help.netlify.app/post/2023-01-14-segmentr-plots/segmentr_plots/","summary":"Answer originally suggested by Jack Penzer, Senior Data Scientist The Problem Sometimes, the plots produced by packages such as SegmentR appear squished with not much white space surrounding the axis text (i.e.¬†the terms for each topic).\nLet‚Äôs use the example ‚ÄòTop Terms‚Äô plot from the SegmentR documentation to see what this means. We start by quickly running through a very simple topic modelling workflow (NB we are not trying different values of min_freq, k_opts etc here because this is just for demonstration- we must try different values for project work)","title":"Change text size of SegmentR plots"},{"content":" Answers originally suggested by Jamie Hudson, Data Scientist and Jack Penzer, Senior Data Scientist The Problem It is important to be able to export/save the figures created in R in a consistent manner, so that throughout a deck all plots of a similar type (e.g.¬†all bigrams) are the same size and resolution.\nIf plots are saved using the Plots panel by clicking Export and Save as image, it is very easy to accidentally save plots as different sizes (and you cannot alter the resolution from the default value).\nSave Panel in RStudio\nThe Solution The way to get around this is via the ggsave() function from the ggplot2 library.\nThis function is super convenient and by default saves the last plot that you have displayed in the Plot pane. It also guesses the type of graphics device from the extension of the saved image.\nThe function works like:\nggsave(filename = \u0026quot;CHANGE_NAME_TO_SOMETHING_SUITABLE.png\u0026quot;, bg = \u0026quot;white\u0026quot;, height = 6, width = 8) where height = 6 and width = 8 tend to be good values to use, and means every plot produced using this code will be the same size ( 6 x 8 ) and makes for nice consistent sizing for decks. As mentioned, by default this function saves the last produced plot (i.e.¬†the one in the Plot pane of RStudio if you have just ran some code to make a plot).\nBut wait, there‚Äôs more Jack‚Äôs produced a neat wrapper function that currently sits in the JPackage library called plot_save(). This function has appropriate defaults for height, width and plot resolution (dpi), so unless we explicitly want these to change, we can just ignore them from the function.\nTherefore to use this function to save a plot, all we need to do is provide the filepath of the location we want to save the plot, with the name of the plot. For example:\nJPackage::plot_save(filename = \u0026quot;drive/folder/project/viz/CHANGE_NAME_TO_SOMETHING_SUITABLE.png\u0026quot;) Sometimes though, the default size of JPackage::plot_save() just won‚Äôt do. For example, you may have many topics from topic modelling and the top term plots all look a bit squished when they‚Äôre saved. In this case, you can try increasing the height and width of the plot by including those arguments such as:\nJPackage::plot_save(filename = \u0026quot;drive/folder/project/viz/CHANGE_NAME_TO_SOMETHING_SUITABLE.png\u0026quot;, height = 9, width = 12) You have to play around with these values until you get a size of plot that is appropriate, üí° but remember to make all of the similar kind of plots the same size for deck/project consistency. üí°\nüéÅ Bonus Tip üéÅ Now, this works great for when we create and want to save a single plot. But for some of the workflows, we produce multiple plots at once (for example the bigrams during LDA topic modelling).\nTo perform this, we need to do some slightly more complex coding which applies a function (i.e.¬†JPackage::plot_save()) across a list or a vector. To show this, let‚Äôs pluck the bigrams from an LDA workflow:\nbigrams \u0026lt;- ldas %\u0026gt;% filter(k == 5, freq_cutoff == 200) %\u0026gt;% SegmentR::explore_LDAs(top_terms = FALSE, diff_terms = FALSE, bigrams = TRUE, bigram_n = 25, exemplars = FALSE) %\u0026gt;% pluck(\u0026#39;bigrams\u0026#39;, 1) This creates a list of the (in this case 5 [because we have filtered for k == 5]) bigrams. Each of these bigrams has a name, and what we can do is append perhaps the value of K onto each of these names using the following code:\nk_val \u0026lt;- 5 # assign a numeric vector called k_val the value of 5 We can view the current names of the bigrams\nnames(bigrams) And then rename them with the appended k value:\nnames(bigrams) \u0026lt;- paste0(names(bigrams), \u0026quot;_\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;_\u0026quot;, k_val) Now we can apply JPackage::plot_save() to the list of bigrams using the function lapply().\nlapply(names(bigrams), function(x) JPackage::plot_save( dir = paste0(\u0026quot;drive/folder/project/viz/bigrams_\u0026quot;, x, \u0026quot;.png\u0026quot;), plot = bigrams[[x]] ) ) What each of these lines does is:\nlapply(names(bigrams) starts a loop that will iterate over the names of the objects in the bigrams list. Note, that if your bigrams list has a different name, it should be changed here too (i.e.¬†lapply(names(DIFFERENT_NAME))).\nfunction(x) is what is known as an anonymous function which takes a single argument known as x, which in this case is an individual name from the bigram list.\nJPackage::plot_save is the function that is called to save the plots\nfilename = paste0(\"drive/folder/project/viz/bigrams_\", x, \".png\") creates a file name for the saved plot abd saves these to the drive/folder/project/viz/ folder (which should be changed to something suitable) and names each bigram in this folder bigram_THE_NAME_OF_THE_BIGRAM.png.\nplot = bigrams[[x]] specifies the plot to be saved. The [[x]] notation retrieves the element of the bigrams list with name x. Again, if the bigram list has a different name, it should be changed here too (i.e.¬†plot = DIFFERENT_NAME[[x]]).\n","permalink":"https://slack-overflow-help.netlify.app/post/2023-01-13-ggsave/ggsave/","summary":"Answers originally suggested by Jamie Hudson, Data Scientist and Jack Penzer, Senior Data Scientist The Problem It is important to be able to export/save the figures created in R in a consistent manner, so that throughout a deck all plots of a similar type (e.g.¬†all bigrams) are the same size and resolution.\nIf plots are saved using the Plots panel by clicking Export and Save as image, it is very easy to accidentally save plots as different sizes (and you cannot alter the resolution from the default value).","title":"Consistently Save Plots"},{"content":" Topic originally suggested by Jack Penzer, Data Scientist The Problem üö®Note this is only relevant to data that has not come from Sprinklr. Quoted retweets are classified as mentions in Sprinklr and only simple retweets are considered retweetsüö®\nWhen creating bigram networks and performing topic modelling on Twitter data it can often be difficult to deal with retweets (RTs).\nIf we leave all RTs in the data set, the bigram networks will comprise only the most retweeted RTs, as will most of the topics inferred through topic modelling.\nHowever, if we blanket remove all RTs, we will also lose the quote RTs, which contain text separate from the RT.\nThe Solution A happy medium is to remove the retweeted part of each quote RT, leaving the novel text to be analysed separately (which will not disrupt our bigram networks and topic modelling processes).\nWarning: check the specific output of RTs first as in the third code block.\nFirst things first, let‚Äôs load the libraries we will be using:\nlibrary(tidyverse) library(janitor) And then load in some sample data, which we will call demo. You will see I also select only a few columns, this is for clarity in visualising the data frame in this tutorial and we do not recommend selecting such few columns for a real-life situation.\ndemo \u0026lt;- read_csv(\u0026quot;data/demo.csv\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% dplyr::select(date, media_type, publisher_username, mention_content) utils::head(20) ## [1] 20 head(demo) ## # A tibble: 6 √ó 4 ## date media_type publisher_username mention_content ## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2022-06-02 Twitter PradoEirin La ilustraci√≥n de hoy en @_Boreal_ p‚Ä¶ ## 2 2022-06-02 Twitter AugustoMetztli La mayonesa, el pan bimbo‚Ä¶ y Get¬†Bac‚Ä¶ ## 3 2022-06-02 Twitter _Boreal_ La mayonesa, el pan bimbo‚Ä¶ y Get¬†Bac‚Ä¶ ## 4 2022-06-02 Twitter ANYWAYSNOW no hay absolutamente nada para desay‚Ä¶ ## 5 2022-06-02 Twitter monnuwrai despu√©s del caldito me com√≠ un pedaz‚Ä¶ ## 6 2022-06-02 Twitter nathmarz A punto de morir por comer unas puta‚Ä¶ In this example, we can see that our text variable is stored in a column called mention_content. For illustrative purposes, let‚Äôs rename this to text and then find us some RTs:\ndemo_RTs \u0026lt;- demo %\u0026gt;% dplyr::rename(text = mention_content) %\u0026gt;% dplyr::select(text) %\u0026gt;% dplyr::filter(str_detect(text, \u0026quot;\\\\bRT\\\\b\u0026quot;)) We use the function str_detect() to search for strings within the text column that match the regex pattern \"\\\\bRT\\\\b. This pattern tells R to find the term ‚ÄúRT‚Äù before and after word boundaries (the transition between word and non-word characters), meaning we will not find words such as ‚ÄúFURTHER‚Äù, as the ‚ÄúRT‚Äù is not next to a word boundary.\nBy viewing demo_RTs, we can see we have two posts that match the filtering criteria:\ndemo_RTs ## # A tibble: 2 √ó 1 ## text ## \u0026lt;chr\u0026gt; ## 1 Se mamaron con los precios yo con un kilo de arroz y una bolsa pan de bimbo e‚Ä¶ ## 2 ¬°Uy, qu√© miedo! ¬øD√≥nde ha estado desde diciembre de 2018? Yo me acuerdo que u‚Ä¶ The eagle-eyed amongst us will notice both of these posts have the quoted text before a ‚Äù RT @‚Äù pattern- so they come with a RT tag followed by the username of the person they‚Äôre retweeting followed by the original tweet.\nNow let‚Äôs mutate a new column and name it rt_info. This will check if the post is a RT, and if it is, take the RT tag and the proceeding text and add it to a new column.\nThen in the next mutate we‚Äôll edit our text column, checking if each post is a RT, and if it is we‚Äôll extract all the text up to the RT tag and forget about the rest - which is stored in rt_info:\ndemo_RTs \u0026lt;- demo_RTs %\u0026gt;% mutate(rt_info = ifelse(str_detect(text, \u0026quot;\\\\bRT\\\\b\u0026quot;), str_match(text, \u0026quot;\\\\bRT\\\\b @.*\u0026quot;), \u0026quot;emptystring\u0026quot;)) %\u0026gt;% mutate(text = ifelse(str_detect(text, \u0026quot;\\\\bRT\\\\b\u0026quot;), str_match(text, \u0026quot;.* \\\\bRT\\\\b\u0026quot;), text)) demo_RTs ## # A tibble: 2 √ó 2 ## text rt_info ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Se mamaron con los precios yo con un kilo de arroz y una bolsa pan de‚Ä¶ RT @jo‚Ä¶ ## 2 ¬°Uy, qu√© miedo! ¬øD√≥nde ha estado desde diciembre de 2018? Yo me acuer‚Ä¶ RT @re‚Ä¶ Now we can see that our text column has two posts, each of them quoted RTs, and our rt_info column includes all information relating to the RT itself. Now we can analyse the quoted RTs, and the original tweets‚Äô information if need be.\nüö®Note leaving the ‚ÄúRT‚Äù at the end of the quoted RTs is intentional, so that posts can still be tracked easily as quote RTs, and in the topic modelling process we‚Äôll still see that the topic comprises a number of quote RTs, but can be removed with mutate (text = str_remove_all(text, \"\\\\bRT\\\\b\"))üö®\n","permalink":"https://slack-overflow-help.netlify.app/post/2022-06-29-retweets/retweets/","summary":"Topic originally suggested by Jack Penzer, Data Scientist The Problem üö®Note this is only relevant to data that has not come from Sprinklr. Quoted retweets are classified as mentions in Sprinklr and only simple retweets are considered retweetsüö®\nWhen creating bigram networks and performing topic modelling on Twitter data it can often be difficult to deal with retweets (RTs).\nIf we leave all RTs in the data set, the bigram networks will comprise only the most retweeted RTs, as will most of the topics inferred through topic modelling.","title":"Extract Comment from Quote Tweet"},{"content":" Topic originally suggested by Jack Penzer, Data Scientist The Problem As bona fide coders we love efficiency, consistency, and reproducibility. In the same way functions enable us to automate common tasks within R in a more reliable way than copy-and-pasting, Code Snippets enable us to quickly insert snippets of code.\nWhat are Code Snippets? Code snippets are templates of small regions of re-usable code, that enable us to enter repeating code patterns easily with consistency.\nExample 1 - load Share packages As a first example, we will create a code snippet that quickly enables us to insert the text that enables us to load all the Share R packages.\nObviously, this can be edited to packages that you know you use more often (e.g.¬†perhaps tidyverse rather than ConnectR)\nGo to Tools \u0026gt; Global Options \u0026gt; Code (or ‚åò , and then Code) Make sure Enable code snippets is ticked. Click on Edit Snippets‚Ä¶ Now scroll to the bottom of the snippet editor and paste the following code: snippet share_libs library(ParseR) library(SegmentR) library(HelpR) library(ConnectR) Note that each line after the snippet line needs to start with a tab.\nNow click Save and Ok to exit the preferences window We are now able to use our snippet. In our R script, RMarkdown document, or console, type in share_libs and enter tab. Our libraries should now appear, and we can run this code as normal to load the libraries.\nExample 2 - formatting R script Another useful example of snippets is to produce a template R script that one can always use when starting a project.\nFor example, it is good practice for R scripts to start with the following information:\nTitle Author Date Date source followed by other sections we know we use in every script, such as loading libraries, reading in data, and cleaning data.\nThis is what we want our snippet to end up looking like:\n# Title: # Author: # Date created: # load libraries ---------------------------------------------------------- # read in data ------------------------------------------------------------ # clean data -------------------------------------------------------------- üí°Note that the # load libraries ----- is what is known as a ‚Äúcode section‚Äù. These sections allow us to break large scripts into discrete regions for easy navigation. These sections are foldable, meaning they have a little arrow next to the line number on the left-hand side of our script, which allows us to collapse this section to make it easy on the eye, if needed. We can insert these sections by pressing ‚åò + Shift + R, and then typing in your label heading.\nNow, back to the creation of the snippet. We could copy and paste the above into the snippet editor as before- however, then we would have to manually select next to ‚ÄúTitle:‚Äù and type our title, then select next to ‚ÄúAuthor:‚Äù and type our name etc. As R is brilliant, it can help with this (remember, we like to automate as many things as people), but must do a little bit of work now to facilitate this.\nFor any section we want to be able to edit quickly, we add the following code:\n${1:NAME_OF_TEXT}\nChanging the number 1 to be the order this appears in the snippet (i.e.¬†in our example 1 = Title, 2 = Author, etc), and NAME_OF_TEXT to be the general name of the bit of information we are coding for (i.e., Title, Name, etc).\nTherefore, our code snippet that we copy and paste into the snippet editor would be:\nsnippet share_script # Title: ${1:Title} # Author: ${2:Name} # Date created: `r Sys.Date()` # load libraries ---------------------------------------------------------- # read in data ------------------------------------------------------------ # clean data -------------------------------------------------------------- üö® Note that for some reason with the way the code above appears on the Slack Overflow website, when you copy and paste it into the snippet editor, you will see red boxes to the side of the hashtags. This is because it thinks we have more than one tab before each line. To fix this, just make sure there is only one tab per line as mentioned before, and the red boxes will disappear. üö® Also, the code 2022-06-30 automatically completes today‚Äôs date for us based on our computers settings.\nNow we should be able to type share_script in a blank R script, and it will automatically population with a nice template. The curser will automatically highlight the Title, so we can just type our title, press tab, and then it will automatically highlight Name, where we can type our Name, and press tab again.\nWe can even combine these two examples so that the R script template automatically includes certain packages in the load libraries sections.\nHappy snippet creating!\n","permalink":"https://slack-overflow-help.netlify.app/post/2022-06-27-creating_snippets/creating_snippets/","summary":"Topic originally suggested by Jack Penzer, Data Scientist The Problem As bona fide coders we love efficiency, consistency, and reproducibility. In the same way functions enable us to automate common tasks within R in a more reliable way than copy-and-pasting, Code Snippets enable us to quickly insert snippets of code.\nWhat are Code Snippets? Code snippets are templates of small regions of re-usable code, that enable us to enter repeating code patterns easily with consistency.","title":"Creating Snippets"},{"content":" Topic originally suggested by Mike Tapp, Data Diretor The Problem Sometimes we need to update our packages, perhaps new functions have been added, patches have been included to fix bugs.\nWe can update packages in a few ways:\nUsing R script If we know a specific package that needs to be updated, we can run the code:\ninstall.packages(\u0026quot;PACKAGE_NAME\u0026quot;) So for example:\ninstall.packages(\u0026quot;ggplot2\u0026quot;) Note the quotation marks surrounding the package name\nUsing the R Package Manager An easier way to update multiple packages is through the Packages window of R Studio (which by default is in the bottom right-hand corner), there is an ‚ÄúUpdate‚Äù button, which you can click to give you a list of packages that are ready to be updated.\nFrom there, we can click ‚ÄúSelect All‚Äù and click ‚ÄúInstall Updates‚Äù.\nWe will then normally be told that R will be restarted during this process. This is normally okay, and R should save everything you are working on- but just in case I would recommend making sure you have everything saved yourself- to be safe.\nTo update Share packages To update the Share packages we can‚Äôt use the Packages pane (as they are hosted privately for us). Therefore, to update these packages as and when required (The DS team will put out a message advising updating when appropriate), we run the same code we originally used to install them in the first place.\ndevtools::install_github(repo = \u0026quot;Avery-Island/ParseR\u0026quot;, auth_token = \u0026quot;let_me_in_please\u0026quot;, force = TRUE) devtools::install_github(repo = \u0026quot;Avery-Island/SegmentR\u0026quot;, auth_token = \u0026quot;let_me_in_please\u0026quot;, force = TRUE) devtools::install_github(repo = \u0026quot;Avery-Island/ConnectR\u0026quot;, auth_token = \u0026quot;let_me_in_please\u0026quot;, force = TRUE) devtools::install_github(repo = \u0026quot;Avery-Island/HelpR\u0026quot;, auth_token = \u0026quot;let_me_in_please\u0026quot;, force = TRUE) Here we just need to make sure we change the auth_token = \"let_me_in_please\" line to an authentication token that you can get from Mike.\nüö®As an aside, once a package has been installed once (through install.packages(\"PACKAGE_NAME\"), we do not need to install it again. It is saved in a directory called ‚Äúlibrary‚Äù in our R environment. Therefore to use these packages in future R sessions, we only need to load them using library(PACKAGE_NAME)üö®\n","permalink":"https://slack-overflow-help.netlify.app/post/2022-06-28-update-packages/update_packages/","summary":"Topic originally suggested by Mike Tapp, Data Diretor The Problem Sometimes we need to update our packages, perhaps new functions have been added, patches have been included to fix bugs.\nWe can update packages in a few ways:\nUsing R script If we know a specific package that needs to be updated, we can run the code:\ninstall.packages(\u0026quot;PACKAGE_NAME\u0026quot;) So for example:\ninstall.packages(\u0026quot;ggplot2\u0026quot;) Note the quotation marks surrounding the package name","title":"Updating Packages"},{"content":" Answer originally provided by Jack Penzer \u0026amp; Jamie Hudson, Data Scientists The Problem Sometimes we have multiple data frames containing the same variables but for different companies, for example. We might then want to be able to join these data frames together, creating a new variable (column) that corresponds to the name of the original data frame.\nIn this example, we are going to use three dummy data frames called amazon, microsoft, and google, which contain the columns message_id, message, and author- though the solutions provided would work even if the data frames had many more columns.\nhead(amazon) ## # A tibble: 3 √ó 3 ## message_id message author ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12345 Amazon was founded in 1994 from my Garage in Bellevue, Wash‚Ä¶ Jeff ‚Ä¶ ## 2 23456 What do you call two monkeys that share an Amazon Account? ‚Ä¶ Tom S‚Ä¶ ## 3 34567 30% of the world\u0026#39;s species live in the Amazon David‚Ä¶ head(microsoft) ## # A tibble: 3 √ó 3 ## message_id message author ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 123 The name Microsoft is a combination of the words microcompu‚Ä¶ Mike ‚Ä¶ ## 2 456 MS Office was released on the Macintosh before the Windows ‚Ä¶ @fact‚Ä¶ ## 3 789 RT @facts_r_us MS Office was released on the Macintosh befo‚Ä¶ @retw‚Ä¶ head(google) ## # A tibble: 3 √ó 3 ## message_id message author ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 22 A Googol is a 1 with 100 zeros after it Larry‚Ä¶ ## 2 33 Check out these amazing Google facts that will blow your mi‚Ä¶ Elgoog ## 3 44 Google is the world\u0026#39;s most visited website Googl‚Ä¶ Solution 1- mutate() and then bind_rows() One solution is to add a new variable to each data frame individually using the dplyr::mutate() function, and then bind these individual data frames into one using the dplyr::bind_rows() function.\nlibrary(dplyr) amazon_mutated \u0026lt;- amazon %\u0026gt;% dplyr::mutate(source = \u0026quot;amazon\u0026quot;) microsoft_mutated \u0026lt;- microsoft %\u0026gt;% dplyr::mutate(source = \u0026quot;microsoft\u0026quot;) google_mutated \u0026lt;- google %\u0026gt;% dplyr::mutate(source = \u0026quot;google\u0026quot;) combined_df \u0026lt;- dplyr::bind_rows(amazon_mutated, microsoft_mutated, google_mutated) Here, we used dplyr::mutate() to create a new column called source, which we labelled with the name of each data frame, before binding using dplyr::bind_rows() to produce our new data frame called combined_df. We can view what this data frame looks like, with our new source column:\nhead(combined_df) ## # A tibble: 6 √ó 4 ## message_id message author source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12345 Amazon was founded in 1994 from my Garage in Bellevu‚Ä¶ Jeff ‚Ä¶ amazon ## 2 23456 What do you call two monkeys that share an Amazon Ac‚Ä¶ Tom S‚Ä¶ amazon ## 3 34567 30% of the world\u0026#39;s species live in the Amazon David‚Ä¶ amazon ## 4 123 The name Microsoft is a combination of the words mic‚Ä¶ Mike ‚Ä¶ micro‚Ä¶ ## 5 456 MS Office was released on the Macintosh before the W‚Ä¶ @fact‚Ä¶ micro‚Ä¶ ## 6 789 RT @facts_r_us MS Office was released on the Macinto‚Ä¶ @retw‚Ä¶ micro‚Ä¶ Solution 2- quicker with gdata::combine() A quicker and simpler way to achieve the same result is using the gdata::combine() function. This combines our data frames into rows of a new data frame with the additional column source, all at once. The value of this new source column is the name of the original (source) data frame.\nN.B. the first time you run this function, you may have to install the gdata library\ninstall.packages(\u0026quot;gdata\u0026quot;) library(gdata) combined_df \u0026lt;- gdata::combine(amazon, microsoft, google) head(combined_df) ## message_id ## 1 12345 ## 2 23456 ## 3 34567 ## 4 123 ## 5 456 ## 6 789 ## message ## 1 Amazon was founded in 1994 from my Garage in Bellevue, Washing ## 2 What do you call two monkeys that share an Amazon Account? Prime mates ## 3 30% of the world\u0026#39;s species live in the Amazon ## 4 The name Microsoft is a combination of the words microcomputer and software ## 5 MS Office was released on the Macintosh before the Windows OS ## 6 RT @facts_r_us MS Office was released on the Macintosh before the Windows OS ## author source ## 1 Jeff Bezos amazon ## 2 Tom Smith amazon ## 3 David Attenborough amazon ## 4 Mike O\u0026#39;Soft microsoft ## 5 @facts_r_us microsoft ## 6 @retweet_bot microsoft Et voil√†, now we have a new data frame with an additional source column with just one line of code!\n","permalink":"https://slack-overflow-help.netlify.app/post/2022-06-10-combine_dfs_with_new_col/gdata_combine/","summary":"Answer originally provided by Jack Penzer \u0026amp; Jamie Hudson, Data Scientists The Problem Sometimes we have multiple data frames containing the same variables but for different companies, for example. We might then want to be able to join these data frames together, creating a new variable (column) that corresponds to the name of the original data frame.\nIn this example, we are going to use three dummy data frames called amazon, microsoft, and google, which contain the columns message_id, message, and author- though the solutions provided would work even if the data frames had many more columns.","title":"Combine data frames with new column"},{"content":" Topic originally suggested by Jack Penzer, Data Scientist The problem Sometimes we might receive data from a social listening output and we want to be able to filter for subreddits of interest.\nFor this tutorial, let‚Äôs say we have a dataset in R called data, that looks like the following:\nhead(data) ## # A tibble: 6 √ó 14 ## project_id date author mention_content ave_sentiment sentiment ## \u0026lt;int\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 2022-04-27 Instagram User love airpods co‚Ä¶ 0.287 POSITIVE ## 2 2 2022-04-10 MrSlowestD16 support apple d‚Ä¶ 0.294 POSITIVE ## 3 3 2022-04-06 SkyRiseElite101 absolutely bril‚Ä¶ 0.358 POSITIVE ## 4 4 2022-04-10 Scott76655335 amazon comau ro‚Ä¶ 0.399 POSITIVE ## 5 5 2022-04-27 rmo777 love google 0.236 POSITIVE ## 6 6 2022-04-28 vigilantebff pretty good hig‚Ä¶ 0.366 POSITIVE ## # ‚Ä¶ with 8 more variables: mention_url \u0026lt;chr\u0026gt;, string_length \u0026lt;int\u0026gt;, ## # word_count \u0026lt;int\u0026gt;, sd \u0026lt;dbl\u0026gt;, text_copy \u0026lt;chr\u0026gt;, apple \u0026lt;lgl\u0026gt;, amazon \u0026lt;lgl\u0026gt;, ## # amend_id \u0026lt;int\u0026gt; Solution Let‚Äôs load in the required packages first:\nlibrary(dplyr) library(ParseR) library(stringr) library(purrr) To do this, firstly we extract the text variable (in this case mention_content) and the URL column (mention_url), using the dplyr::select() function and store these in a new data frame.\ncompare_subreddits \u0026lt;- data %\u0026gt;% dplyr::select(mention_content, mention_url) head(compare_subreddits) ## # A tibble: 6 √ó 2 ## mention_content mention_url ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 love airpods convenient biggest unlike generations case recharges‚Ä¶ https://ww‚Ä¶ ## 2 support apple decision support vulkan developer write graphics dr‚Ä¶ https://ww‚Ä¶ ## 3 absolutely brilliant amd intel finally start trade blows hit lowe‚Ä¶ https://ww‚Ä¶ ## 4 amazon comau robotics system building cars im pretty tesla https://tw‚Ä¶ ## 5 love google https://tw‚Ä¶ ## 6 pretty good highly recommend loved freddie hp bridgerton https://tw‚Ä¶ The next step is to write a RegEx pattern that extracts the name of the subreddits from our mention_url column (in the form of ‚Äò/r/subreddit‚Äô), using stringr::str_extract. These named subreddits are placed in a new column called subreddits using dplyr::mutate. Finally for this step, we filter out the rows where the subreddits column is NA (i.e.¬†there was no subreddit found within the mention_url column) using dplyr::filter().\ncompare_subreddits \u0026lt;- compare_subreddits %\u0026gt;% dplyr::mutate(subreddits = stringr::str_extract(mention_url, \u0026quot;/r/\\\\w+/\u0026quot;))%\u0026gt;% dplyr::filter(!is.na(subreddits)) head(compare_subreddits) ## # A tibble: 6 √ó 3 ## mention_content mention_url subreddits ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 install linux ubuntu popos dual boot convince things e‚Ä¶ https://ww‚Ä¶ /r/thinkp‚Ä¶ ## 2 nato buy titanium band amazon ll love titanium band fi‚Ä¶ https://ww‚Ä¶ /r/OmegaW‚Ä¶ ## 3 found facebook love nanny degree ece previously worked‚Ä¶ https://ww‚Ä¶ /r/bayare‚Ä¶ ## 4 cooling blankets shiny striped bamboo amazon shiny wor‚Ä¶ https://ww‚Ä¶ /r/Menopa‚Ä¶ ## 5 amazon approved exception return product year late tho‚Ä¶ https://ww‚Ä¶ /r/boardg‚Ä¶ ## 6 mount sigma mm f extremely sharp favorite shot sony ap‚Ä¶ https://ww‚Ä¶ /r/fujifi‚Ä¶ Now we can start to do whatever analysis is required, for example we could count the number of posts from each subreddit in our data frame using dplyr::count()‚Ä¶\ncompare_subreddits %\u0026gt;% dplyr::count(subreddits, sort = TRUE) ## # A tibble: 9,279 √ó 2 ## subreddits n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 /r/apple/ 956 ## 2 /r/pcmasterrace/ 841 ## 3 /r/mac/ 671 ## 4 /r/AskReddit/ 623 ## 5 /r/ipad/ 621 ## 6 /r/macbookpro/ 546 ## 7 /r/linuxmasterrace/ 531 ## 8 /r/thinkpad/ 475 ## 9 /r/buildapc/ 457 ## 10 /r/linux_gaming/ 438 ## # ‚Ä¶ with 9,269 more rows ‚Ä¶ or we might want to filter out unwanted posts from questionable subreddits (such as /r/wallstreetbets) using dplyr::filter‚Ä¶\ncompare_subreddits %\u0026gt;% dplyr::filter(!subreddits == \u0026quot;/r/wallstreetbets/\u0026quot;) ## # A tibble: 59,361 √ó 3 ## mention_content mention_url subreddits ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 install linux ubuntu popos dual boot convince things ‚Ä¶ https://ww‚Ä¶ /r/thinkp‚Ä¶ ## 2 nato buy titanium band amazon ll love titanium band f‚Ä¶ https://ww‚Ä¶ /r/OmegaW‚Ä¶ ## 3 found facebook love nanny degree ece previously worke‚Ä¶ https://ww‚Ä¶ /r/bayare‚Ä¶ ## 4 cooling blankets shiny striped bamboo amazon shiny wo‚Ä¶ https://ww‚Ä¶ /r/Menopa‚Ä¶ ## 5 amazon approved exception return product year late th‚Ä¶ https://ww‚Ä¶ /r/boardg‚Ä¶ ## 6 mount sigma mm f extremely sharp favorite shot sony a‚Ä¶ https://ww‚Ä¶ /r/fujifi‚Ä¶ ## 7 great running google mesh router google nest hub add ‚Ä¶ https://ww‚Ä¶ /r/homeau‚Ä¶ ## 8 wondering copy redhat unlimited updates customer supp‚Ä¶ https://ww‚Ä¶ /r/linuxm‚Ä¶ ## 9 great gmail hangouts easy imo https://ww‚Ä¶ /r/Minecr‚Ä¶ ## 10 knowledge helios bad good lenovo legion y legion disa‚Ä¶ https://ww‚Ä¶ /r/Sugges‚Ä¶ ## # ‚Ä¶ with 59,351 more rows ‚Ä¶ or we might want to perform a WLO to distinctness in conversations between subreddits. Say we want to compare /r/pcmasterrace, /r/mac, and /r/linuxmasterrace/, we can dplyr::filter() for these subreddits within the subreddits column, and pipe (%\u0026gt;%) this into the ParseR::calculate_wlos function, where our topic_var is the subreddits column and our text_var is the mention_content column.\ncompare_subreddits %\u0026gt;% dplyr::filter(subreddits %in% c(\u0026quot;/r/mac/\u0026quot;,\u0026quot;/r/pcmasterrace/\u0026quot;, \u0026quot;/r/linuxmasterrace/\u0026quot;)) %\u0026gt;% ParseR::calculate_wlos(topic_var = subreddits, text_var = mention_content, top_n = 15) %\u0026gt;% purrr::pluck(\u0026quot;viz\u0026quot;) ","permalink":"https://slack-overflow-help.netlify.app/post/2022-06-10-subreddit_extraction/subreddit_extraction/","summary":"Topic originally suggested by Jack Penzer, Data Scientist The problem Sometimes we might receive data from a social listening output and we want to be able to filter for subreddits of interest.\nFor this tutorial, let‚Äôs say we have a dataset in R called data, that looks like the following:\nhead(data) ## # A tibble: 6 √ó 14 ## project_id date author mention_content ave_sentiment sentiment ## \u0026lt;int\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 2022-04-27 Instagram User love airpods co‚Ä¶ 0.","title":"Extracting and filtering for subreddits"},{"content":" Answer originally provided by Jack Penzer, Data Scientist The Problem When trying to plot a visualisation (e.g., bigrams), you are greeted with the following error message:\nType of error message we may see\nThis is an issue with R being unable to find the appropriate fonts to produce our plot.\nThe Solution- reinstall fonts and restart R session. We need to make sure we have the correct fonts installed on our computers, and also imported into R.\nThe fonts that we need to make sure we have installed are:\n- ‚ÄúNeue Haas Grotesk Text Pro 55 Roman‚Äù - ‚ÄúNeue Haas Grotesk Text Pro 65 Medium‚Äù - ‚ÄúNeue Haas Grotesk Text Pro 75 Bold‚Äù.\nWe can check this by opening our Font Book application on our Macs. To do this, press ‚åò + space and type ‚ÄúFont Book‚Äù, and it should appear. We can then double check that these fonts appear:\nList of fonts as they appear in Font Book\nIf we do not have these fonts, they can be downloaded from enter link here.\nNow, we can run the following code in R to rebook our fonts and make sure all our system fonts (from Font Book) are imported into R.\nlibrary(extrafont) font_import() loadfonts() Now a quick reboot of R (you can do this by clicking the ‚ÄúSession‚Äù from the RStudio menu bar and choosing ‚ÄúRestart R‚Äù) should enable you to continue plotting to your hearts content.\nN.B. Before restarting R, make sure you save any large datasets (e.g., ldas, explore object) and/or scripts you may be working on! As a recommendation, save the datasets as an .rds file using the readr::write_rds() function\n","permalink":"https://slack-overflow-help.netlify.app/post/2022-06-10-issues_with_fonts/issues_with_fonts/","summary":"Answer originally provided by Jack Penzer, Data Scientist The Problem When trying to plot a visualisation (e.g., bigrams), you are greeted with the following error message:\nType of error message we may see\nThis is an issue with R being unable to find the appropriate fonts to produce our plot.\nThe Solution- reinstall fonts and restart R session. We need to make sure we have the correct fonts installed on our computers, and also imported into R.","title":"Issues with fonts"},{"content":" Answer originally provided by Mike Tapp, Data Director The Problem Quite often, we might have data spread across multiple files. This could be multiple listening exports, survey responses etc. It would be quite time consuming to read them all in one by one. In this example, we‚Äôre going to look at a solution at is able to extract the file names from a directory and then read in the files that match the strings you have extracted in one go using the map() function.\nExtracting the names R will always need to know the exact names of the files you are trying to read in. You could sit there and copy and paste the file names, but what if you had 20+ files? It‚Äôs not very efficient. Instead, let R do the hard work for you and find the names itself.\nLet‚Äôs look at the recent Adidas pitch data. There were only two files, but this principle will work no matter how many there are. First, we update our working directory so R knows where the files are saved. Then, as we know all our files are CSVs, we ask R to create an object called paths which is a list of all the CSV files in the directory.\nsetwd(\u0026quot;/Volumes/GoogleDrive/My Drive/Share_Clients/Data Science Project Work/Slack Overflow/reading_multiple_files/data\u0026quot;) paths \u0026lt;- list.files(pattern = \u0026quot;csv$\u0026quot;, recursive = TRUE) paths ## [1] \u0026quot;Adidas - Feb - Apr .csv\u0026quot; \u0026quot;Adidas - Nov - Jan .csv\u0026quot; R has correctly identified the two datafiles in the path. Now it‚Äôs time to read them in.\nUsing map() to call the same function on multiple objects map() is a special function that allows you to conduct another function over multiple objects at the same time. When you think about it, it‚Äôs exactly the kind of thing we need here. We need to call read_csv() multiple times and map() is going to save us so much time by doing all of those processes at once. The code to do so is very simple:\nall_docs \u0026lt;- purrr::map(paths, read_csv) R has created an object called all_docs which is the result reading in all the CSVs. Great! We aren‚Äôt quite there though because at the moment, whilst all the data is in our R environment, they are living as separate objects in that list. We need to bind them all together. For that, we can use a full_join()\ndata \u0026lt;- purrr:::reduce(all_docs, full_join) ## Joining, by = c(\u0026quot;date\u0026quot;, \u0026quot;text\u0026quot;, \u0026quot;title\u0026quot;, \u0026quot;permalink\u0026quot;, \u0026quot;site name\u0026quot;, \u0026quot;queries\u0026quot;, ## \u0026quot;lang\u0026quot;, \u0026quot;tone\u0026quot;, \u0026quot;emotion(s)\u0026quot;, \u0026quot;platform\u0026quot;, \u0026quot;post type\u0026quot;, \u0026quot;source name\u0026quot;, \u0026quot;author\u0026quot;, ## \u0026quot;screen name\u0026quot;, \u0026quot;gender\u0026quot;, \u0026quot;avatar\u0026quot;, \u0026quot;verified author account\u0026quot;, \u0026quot;inferred ## country\u0026quot;, \u0026quot;inferred region\u0026quot;, \u0026quot;inferred city\u0026quot;, \u0026quot;inferred longitude\u0026quot;, \u0026quot;inferred ## latitude\u0026quot;, \u0026quot;inferred country population\u0026quot;, \u0026quot;inferred region population\u0026quot;, ## \u0026quot;inferred city population\u0026quot;, \u0026quot;declared country\u0026quot;, \u0026quot;declared region\u0026quot;, \u0026quot;declared ## city\u0026quot;, \u0026quot;declared longitude\u0026quot;, \u0026quot;declared latitude\u0026quot;, \u0026quot;declared country ## population\u0026quot;, \u0026quot;declared region population\u0026quot;, \u0026quot;declared city population\u0026quot;, \u0026quot;twitter ## author id\u0026quot;, \u0026quot;twitter followers\u0026quot;, \u0026quot;twitter retweets\u0026quot;, \u0026quot;twitter retweed of\u0026quot;, ## \u0026quot;twitter replies\u0026quot;, \u0026quot;twitter likes\u0026quot;, \u0026quot;facebook author id\u0026quot;, \u0026quot;facebook type\u0026quot;, ## \u0026quot;facebook fans\u0026quot;, \u0026quot;facebook likes\u0026quot;, \u0026quot;facebook shares\u0026quot;, \u0026quot;facebook comments\u0026quot;, ## \u0026quot;facebook last update of the likes / shares / comments\u0026quot;, \u0026quot;instagram author id\u0026quot;, ## \u0026quot;instagram followers\u0026quot;, \u0026quot;instagram likes\u0026quot;, \u0026quot;instagram comments\u0026quot;, \u0026quot;instagram ## replies\u0026quot;, \u0026quot;instagram taps back\u0026quot;, \u0026quot;instagram taps forward\u0026quot;, \u0026quot;instagram exits\u0026quot;, ## \u0026quot;impressions (IG numbers)\u0026quot;, \u0026quot;reach (IG numbers)\u0026quot;, \u0026quot;instagram last update of the ## likes / comments\u0026quot;, \u0026quot;pinterest author ID\u0026quot;, \u0026quot;pinterest followers\u0026quot;, \u0026quot;pinterest ## saves\u0026quot;, \u0026quot;pinterest photos\u0026quot;, \u0026quot;pinterest last update of the saves / photos\u0026quot;, \u0026quot;web ## pagerank\u0026quot;, \u0026quot;web inbound links\u0026quot;, \u0026quot;web shares on twitter\u0026quot;, \u0026quot;web comments\u0026quot;, ## \u0026quot;sinaweibo author ID\u0026quot;, \u0026quot;sinaweibo followers\u0026quot;, \u0026quot;sinaweibo reposts\u0026quot;, \u0026quot;sinaweibo ## repost of\u0026quot;, \u0026quot;sinaweibo likes\u0026quot;, \u0026quot;sinaweibo comments\u0026quot;, \u0026quot;wechat author ID\u0026quot;, ## \u0026quot;wechat wow\u0026quot;, \u0026quot;wechat likes\u0026quot;, \u0026quot;wechat reads\u0026quot;, \u0026quot;wechat last update of the Likes ## / Reads\u0026quot;, \u0026quot;tiktok author ID\u0026quot;, \u0026quot;tiktok followers\u0026quot;, \u0026quot;tiktok views\u0026quot;, \u0026quot;tiktok ## likes\u0026quot;, \u0026quot;tiktok shares\u0026quot;, \u0026quot;tiktok comments\u0026quot;, \u0026quot;tiktok last update of the views / ## likes / shares / comments\u0026quot;, \u0026quot;douyin author ID\u0026quot;, \u0026quot;douyin likes\u0026quot;, \u0026quot;douyin ## comments\u0026quot;, \u0026quot;douyin last update of the likes / comments\u0026quot;, \u0026quot;red likes\u0026quot;, \u0026quot;red ## comments\u0026quot;, \u0026quot;red stars\u0026quot;, \u0026quot;red last update of the likes / comments / stars\u0026quot;, ## \u0026quot;youtube author ID\u0026quot;, \u0026quot;youtube views\u0026quot;, \u0026quot;youtube likes\u0026quot;, \u0026quot;youtube dislikes\u0026quot;, ## \u0026quot;youtube comments\u0026quot;, \u0026quot;youtube favourites\u0026quot;, \u0026quot;youtube last update of views\u0026quot;, ## \u0026quot;dailymotion author ID\u0026quot;, \u0026quot;dailymotion views\u0026quot;, \u0026quot;dailymotion comments\u0026quot;, ## \u0026quot;dailymotion last update of views\u0026quot;, \u0026quot;vkontakte author ID\u0026quot;, \u0026quot;vkontakte ## Fans/Members\u0026quot;, \u0026quot;vkontakte likes\u0026quot;, \u0026quot;vkontakte comments\u0026quot;, \u0026quot;vkontakte shares\u0026quot;, ## \u0026quot;vkontakte last update of likes / comments / shares\u0026quot;, \u0026quot;reviews Radarly ## normalized rating\u0026quot;, \u0026quot;reviews platform rating\u0026quot;, \u0026quot;impressions\u0026quot;, \u0026quot;estimated ## reach\u0026quot;, \u0026quot;engagement actions\u0026quot;, \u0026quot;embedded url\u0026quot;, \u0026quot;media type\u0026quot;, \u0026quot;media url\u0026quot;, \u0026quot;image ## analysis_People detected\u0026quot;, \u0026quot;image analysis_Gender detected\u0026quot;, \u0026quot;image ## analysis_Object(s) and animal(s) detected\u0026quot;, \u0026quot;image analysis_Logo(s) detected\u0026quot;, ## \u0026quot;favorite\u0026quot;, \u0026quot;keywords\u0026quot;, \u0026quot;hashtags\u0026quot;, \u0026quot;mentions\u0026quot;, \u0026quot;named entities\u0026quot;, \u0026quot;story\u0026quot;, ## \u0026quot;topics\u0026quot;, \u0026quot;content classification\u0026quot;, \u0026quot;Custom field : TNF High End\u0026quot;, \u0026quot;Custom ## field : Gore-Tex\u0026quot;, \u0026quot;Custom field : Technical Solutions\u0026quot;, \u0026quot;Custom field : ## Brands\u0026quot;, \u0026quot;Custom field : Sports\u0026quot;, \u0026quot;Custom field : Moshi Topics\u0026quot;, \u0026quot;Custom field ## : Ad Posts\u0026quot;, \u0026quot;Custom field : Profanity2\u0026quot;, \u0026quot;Custom field : Occasions\u0026quot;, \u0026quot;Custom ## field : NeedStates\u0026quot;, \u0026quot;Custom field : Interests\u0026quot;) data ## # A tibble: 100,000 √ó 140 ## date text title permalink `site name` queries lang tone `emotion(s)` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 08/02/202‚Ä¶ \u0026quot;Hap‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ joy ## 2 02/03/202‚Ä¶ \u0026quot;Hap‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ joy ## 3 16/03/202‚Ä¶ \u0026quot;Hap‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ joy ## 4 16/04/202‚Ä¶ \u0026quot;Loo‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ \u0026lt;NA\u0026gt; ## 5 27/04/202‚Ä¶ \u0026quot;Hum‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ joy ## 6 30/03/202‚Ä¶ \u0026quot;It‚Äô‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ \u0026lt;NA\u0026gt; ## 7 15/02/202‚Ä¶ \u0026quot;Hap‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ joy ## 8 09/03/202‚Ä¶ \u0026quot;Hap‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ joy ## 9 23/02/202‚Ä¶ \u0026quot;Hap‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ \u0026lt;NA\u0026gt; ## 10 09/03/202‚Ä¶ \u0026quot;üö®‚Ä¶ \u0026lt;NA\u0026gt; https://‚Ä¶ http://ins‚Ä¶ Adidas en posi‚Ä¶ \u0026lt;NA\u0026gt; ## # ‚Ä¶ with 99,990 more rows, and 131 more variables: platform \u0026lt;chr\u0026gt;, ## # `post type` \u0026lt;chr\u0026gt;, `source name` \u0026lt;lgl\u0026gt;, author \u0026lt;chr\u0026gt;, `screen name` \u0026lt;chr\u0026gt;, ## # gender \u0026lt;chr\u0026gt;, avatar \u0026lt;chr\u0026gt;, `verified author account` \u0026lt;chr\u0026gt;, ## # `inferred country` \u0026lt;chr\u0026gt;, `inferred region` \u0026lt;chr\u0026gt;, `inferred city` \u0026lt;chr\u0026gt;, ## # `inferred longitude` \u0026lt;dbl\u0026gt;, `inferred latitude` \u0026lt;dbl\u0026gt;, ## # `inferred country population` \u0026lt;dbl\u0026gt;, `inferred region population` \u0026lt;dbl\u0026gt;, ## # `inferred city population` \u0026lt;dbl\u0026gt;, `declared country` \u0026lt;chr\u0026gt;, ‚Ä¶ So, there were are - all the data exports loaded in and bound into one object quickly and efficiently!\n","permalink":"https://slack-overflow-help.netlify.app/post/2022-05-20-reading_multiple_files/reading-in-multiple-data-files-at-once/","summary":"Answer originally provided by Mike Tapp, Data Director The Problem Quite often, we might have data spread across multiple files. This could be multiple listening exports, survey responses etc. It would be quite time consuming to read them all in one by one. In this example, we‚Äôre going to look at a solution at is able to extract the file names from a directory and then read in the files that match the strings you have extracted in one go using the map() function.","title":"Reading in Multiple Data Files at Once"},{"content":"HelpR ParseR SegmentR ConnectR LimpiaR- GitHub site ","permalink":"https://slack-overflow-help.netlify.app/share_packages/","summary":"SHARE suite","title":"SHARE Packages"}]